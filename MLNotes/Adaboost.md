adaboost（Adaptive Boosting自适应提升），是一种集成学习技术。他的自适应体现在：被前一个弱分类器错误分类的样本的权值会增大，正确分类的样本的权值会减小，用于训练下一个弱分类器。最终的分类器是由这几个弱分类器的加权和组成。



模型：
$$
H(x) = sign(\sum_{i=1}^n \alpha_ih_i(x))
$$
其中$h_i(x)$代表第i个弱分类器， $\alpha_i$代表第i个弱分类器的权重。有几次迭代训练，就会产生几个弱分类器。

每次迭代完后，对应产生的弱分类器的参数将不再发生变化。



训练过程：

![Adaboost训练过程](/Users/admin/Documents/MLNotes/Adaboost训练过程.png)



#### 1.初始化样本数据的权重

一开始每个训练样本都被赋予相同的权重：$w_i=\frac{1}{N}$，N代表样本数量。



#### 开始迭代，直至强学习器的误差低于某个值或者达到最大迭代次数

##### 1 初始化样本数据的权重

一开始每个训练样本都被赋予相同的权重：$w_i=\frac{1}{N}$，N代表样本数量。

##### 2.根据错误率，选择最优弱学习器

错误率计算公式：
$$
e_m = \sum_{i=1}^nw_{mi}I(h_i(x_i) \neq y_i)
$$
m代表迭代次数，I(x)指示函数，当变量为true时，I(x)=1；当变量为false时，I(x)=0。$h_i()$代表第m轮迭代的弱分类器，$w_{mi}$代表第m轮迭代时第i个样本的权重。

##### 3.计算最优弱分类器的权重

权重计算公式：
$$
\alpha_i = \frac{1}{2}ln(\frac{1-e_i}{e_i})
$$
其中$e_i$代表错误率。可以看出，错误率越低，此弱分类器的权重越大。



##### 4.根据错误率更新样本权重

$$
w_{m+1, i} = \frac{w_{mi} * exp(-\alpha_my_ih_m(x_i))}{\sum_{i=1}^{n}w_{mi} * exp(-\alpha_my_ih_m(x_i))}
$$

m代表第m轮迭代，n代表样本数量，分母一大坨是用来作归一化处理的。可以看出，当预测值$h_m(x_i)$与真实值$y_i$一致时，第m+1轮迭代时，对应的权重会降低；反之会升高。













































