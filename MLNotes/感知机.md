

wx+b=0

w是超平面的法向量。：



在超平面上任意2点，x1和x2，有：
$$
\omega x_1 + b = 0 \\
\omega x_2 + b = 0
$$
 两式相减：
$$
\omega(x_1 - x_2) = 0
$$
x1-x2是超平面中的任意向量，说明W和超平面中的任意向量都垂直，所以W是超平面的法向量





距离表达式：

x0为距离超平面wx+b=0 为d的一个点，假设x0在超平面上的投影是x1，因为向量x0x1与法向量w平行，那么：
$$
\begin{aligned} 
|\vec{x_0x_1} \cdot \vec{\omega}| &= |\vec{x_0x_1}|\cdot|\vec{\omega}| \\ 
&= d \cdot ||\vec{\omega}||
\end{aligned}
$$
另外向量x0x1与法向量w乘积还可以表示为：
$$
\begin{aligned} 
|\vec{x_0x_1} \cdot \vec{\omega}| &= |(\vec{x_0}-\vec{x_1})\cdot \vec{\omega}| \\
&= |x_0\omega - x_1\omega| \\ 
&= |(x_0\omega + b) - (x_1\omega + b)| \\ 
&= |x_0\omega + b|
\end{aligned}
$$
所以：
$$
d = \frac{|x_0\omega + b|}{||\vec{\omega}||}
$$


也可以根据距离表达式
$$
d = |\frac{w_0x_0 + w_1x_1 + ... +w_nx_n + b}{\sqrt{w_0^2 + w_1^2 + ... +w_n^2}}|
$$


直接得到



模型：

感知机可用来实现二分类。模型输入空间为n维向量，n代表特征个数，输出空间为{-1, 1}分别代表两个分类。函数表达式为：
$$
f(x) = sign(w\cdot x + b)
$$
其中sign为符号函数，当wx+b>0时，sign(x)=1;当wx+b<0时，sign(x)=-1。

模型的几何解释为：

![感知机](/Users/admin/Documents/MachineLearningTutorial/MLNotes/感知机.webp)

即当wx+b>0时（x在超平面的上方），y=1；当wx+b<0时（x在超平面的下方），y=-1



损失函数：

感知机使用**误分类点到超平面的距离的累加和**作为损失函数，如果没有误分类点，则损失函数为0

当一个误分类点时：
$$
L_0 =\begin{cases} 
\frac{x_0\omega + b}{||\vec{\omega}||},  & \hat{y_0}=-1\\
\frac{-(x_0\omega + b)}{||\vec{\omega}||}, & \hat{y_0}=1
\end{cases}
$$
整理可以得到：
$$
L_0 = \frac{-y_0(x_0\omega + b)}{||\vec{\omega}||}
$$
n个误分类点到超平面的总距离为：
$$
L = \frac{1}{||w||}\sum_{i=0}^{n}-y_i(x_i\omega + b)
$$
实际可以去掉1/w，因为感知机是误分类驱动的，只和正负有关，不需要计算实际距离；而且去掉1/w对求导也有好处。去掉1/w后的损失函数为:
$$
L = \sum_{i=0}^{n}-y_i(x_i\omega + b)
$$




优化算法：梯度下降













































