决策树构建过程：

### ID3算法

根据信息增益来选择进行划分的特征，选择信息增益最大的特征来作为节点划分特征。

信息增益 = 信息熵 - 条件熵

信息熵：**所有可能发生的事件带来的信息量的期望值**
$$
Entropy(X) = - \sum_{i=1}^{n}p(x_i)logp(x_i)
$$
其中$p(x_i)$代表分类$x_i$出现的概率。n是分类的数目。

信息的大小与随机事件的概率有关。发生概率越小的事件产生的信息越大。





条件熵：**定义为X给定条件下，Y的条件概率分布的熵对X的数学期望**。实际应用时，Y为分类数目，X为特征
$$
\begin{aligned}
Entropy(Y|X) &= \sum_{i=1}^{n}p(x_i)Entropy(Y|x_i) \\
&= - \sum_{x \in X}p(x)\sum_{y \in Y}p(y|x)log(y|x) \\ 
&= - \sum_{x \in X}\sum_{y \in Y}p(x, y)log(y|x)
\end{aligned}
$$


属性A对数据集D的信息增益为$infoGain(D|A)$，它等于D本身的熵减去在给定属性A的条件下的条件熵。
$$
infoGain(D|A) = Entropy(D) - Entropy(D|A)
$$
其中A取值范围为$[a_0, a_1,a_2,...,a_k]$，k个值



附：条件熵计算示例：[条件熵计算示例](https://blog.csdn.net/qq_40765537/article/details/114838485)





### C4.5算法

使用信息增益进行节点特征选择的缺陷：特征可取值比较多的时候，信息增益比较大

c4.5算法使用信息增益率在进行节点的特征选取。
$$
infoGain\_ratio(D, a) = \frac{infoGain(D, a)}{IV(a)}
$$



其中：
$$
IV(a) = -\sum_{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D_v|}{|D|}
$$
$IV(a)$即特征a的信息熵







### Cart分类树算法

cart分类树算法使用基尼系数来选择特征。基尼系数越小，纯度越高，特征越好。与信息增益（率）相反。cart算法建立起来的树是二叉树，而不是多叉树
$$
\begin{aligned}
Gini(D) &= \sum_{i=1}^{n}p(x_i)(1-p(x_i)) \\
&= 1 - \sum_{i=1}^np(x_i)^2
\end{aligned}
$$
其中n是分类的数目，$p(x_i)$是分类$x_i$出现的概率。



在属性A的条件下，样本D的基尼系数定义为：
$$
GiniIndex(D|A=a) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)
$$


分类树建树过程：

1. 如果样本个数少于阈值或者没有特征，则返回决策子树，当前节点停止递归
2. 计算样本D的基尼系数，如果基尼系数小于阈值，则返回决策子树，当前节点停止递归
3. 计算当前节点**各个特征**的**各个值**的基尼系数，选择所有特征中最小基尼系数的特征以及其切分值作为当前节点最优特征和最优切分点。
4. 继续递归处理切分后的$D_1$和$D_2$



cart分类树建树示例：[cart建树示例](https://zhuanlan.zhihu.com/p/139523931)







### Cart回归树算法

cart回归树使用最小化误差平方和来选择特征。
$$
min_{A, a}[min_{c_1}\sum_{x_i \in D_1(A, a)}(y_i-c_1)^2 + min_{c_2}\sum_{x_i \in D_2(A, a)}(y_i-c_2)^2]
$$
其中$D_1$和$D_2$代表用特征A=a进行划分后的两个子样本集。
$c_1$为按照A=a进行划分后$D_1$样本的均值，$c_2$为按照A=a进行划分后$D_2$样本的均值。

$y_i$代表A=a时对应的实际值。



建树过程：同时寻找特征A和对应的属性值a，使得用A=a进行划分时，所获得的两个子集的所有样本点的平方误差之和最小。误差为每个子集的每个样本点对应的输出值减去当前样本点的输出值

预测值为叶子节点的样本输出点的均值。



参考：[决策树算法--CART回归树算法](https://zhuanlan.zhihu.com/p/139519852)

建树过程示例：[CART-回归树 实例](https://blog.csdn.net/liuhe2296044/article/details/112723466)









































































