决策树构建过程：

### ID3算法

根据信息增益来选择进行划分的特征，选择信息增益最大的特征来作为节点划分特征。

信息增益 = 信息熵 - 条件熵

信息熵：**所有可能发生的事件带来的信息量的期望值**
$$
Entropy(X) = - \sum_{i=1}^{n}p(x_i)logp(x_i)
$$
其中$p(x_i)$代表分类$x_i$出现的概率。n是分类的数目。

信息的大小与随机事件的概率有关。发生概率越小的事件产生的信息越大。





条件熵：**定义为X给定条件下，Y的条件概率分布的熵对X的数学期望**。实际应用时，Y为分类数目，X为特征
$$
\begin{aligned}
Entropy(Y|X) &= \sum_{i=1}^{n}p(x_i)Entropy(Y|x_i) \\
&= - \sum_{x \in X}p(x)\sum_{y \in Y}p(y|x)log(y|x) \\ 
&= - \sum_{x \in X}\sum_{y \in Y}p(x, y)log(y|x)
\end{aligned}
$$


属性A对数据集D的信息增益为$infoGain(D|A)$，它等于D本身的熵减去在给定属性A的条件下的条件熵。
$$
infoGain(D|A) = Entropy(D) - Entropy(D|A)
$$
其中A取值范围为$[a_0, a_1,a_2,...,a_k]$，k个值



条件熵计算示例：[条件熵计算示例](https://blog.csdn.net/qq_40765537/article/details/114838485)





### C4.5算法

使用信息增益进行节点特征选择的缺陷：特征可取值比较多的时候，信息增益比较大

c4.5算法使用信息增益率在进行节点的特征选取。
$$
infoGain\_ratio(D, a) = \frac{infoGain(D, a)}{IV(a)}
$$













































